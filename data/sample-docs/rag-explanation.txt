RAG(Retrieval-Augmented Generation)는 검색 증강 생성이라고 불리는 AI 기술입니다.

RAG의 핵심 원리:
LLM(대규모 언어 모델)이 답변을 생성하기 전에, 먼저 관련 문서를 검색하여 컨텍스트로 제공합니다. 이를 통해 LLM이 학습하지 않은 최신 정보나 특정 도메인 지식에 대해서도 정확하게 답변할 수 있습니다.

RAG 파이프라인 구성:
1. 문서 수집(Ingestion): 원본 문서를 작은 청크(chunk)로 분할합니다.
2. 임베딩 생성: 각 청크를 벡터(숫자 배열)로 변환합니다. OpenAI의 text-embedding-3-small 모델이 대표적입니다.
3. 벡터 저장: 생성된 벡터를 pgvector 같은 벡터 데이터베이스에 저장합니다.
4. 쿼리 처리: 사용자 질문도 임베딩으로 변환한 뒤, 코사인 유사도로 가장 관련 있는 청크를 검색합니다.
5. 응답 생성: 검색된 청크를 시스템 프롬프트에 포함시켜 LLM이 근거 있는 답변을 생성합니다.

RAG의 장점:
- 환각(hallucination) 감소: 실제 문서에 기반하여 답변하므로 거짓 정보 생성이 줄어듭니다.
- 최신 정보 반영: 문서를 업데이트하면 즉시 반영됩니다.
- 출처 추적 가능: 어떤 문서에서 정보를 가져왔는지 표시할 수 있습니다.
- 비용 효율적: 모델을 재학습(fine-tuning)하지 않아도 됩니다.